{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d068efa",
   "metadata": {},
   "source": [
    "* Written and coded by: Chirag Mirani\n",
    "* On Tuesday, January 25, 2022\n",
    "\n",
    "# Solving OpenAI Taxi environment with Q-Learning (Reinforcement Learning)\n",
    "* Taxi is one of the simpler environments available on OpenAI Gym. \n",
    "* The goal of the Autonomous Taxi is to drop off the passenger in the correct location in the least amount of time. \n",
    "* In numbers term, agent loses one point for each step and gains 20 points for dropping the passeger off to the correct location. There is also a 10 point penalty for illegal pick-up and drop-off actions.\n",
    "* We train a random agent first. \n",
    "* Next, we are going to train our agent to be a better taxi driver using Q-learning. \n",
    "* Q-learning or reinforcement learning, training an 'agent'to learn the correct sequences of actions in its environment as to maximise its reward. \n",
    "* In the process, I hope to better understand q-learning. \n",
    "\n",
    "\n",
    "\n",
    "# 1. Importing Required Libraries\n",
    "* Key library here is Openai Gym, which allows us to import OpenAI environments\n",
    "* This \"from IPython.display import clear_output\" is used to clear output from Jupyter Notebook\n",
    "* make sure to install gym and numpy\n",
    "* OpenAI Gym install command: pip install gym\n",
    "* NumPy install command: pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "42fbd529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d541a9ac",
   "metadata": {},
   "source": [
    "# 2. Importing Taxi-v3 environment\n",
    "This task was introduced in [Dietterich2000] to illustrate some issues in hierarchical reinforcement learning. There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\n",
    "* **TIP**: This method should also work with FrozenLake environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1d997d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "#env = gym.make('FrozenLake-v1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad517d8",
   "metadata": {},
   "source": [
    "# 3. Random Agent: Goes through ten episodes in the Taxi enviroment\n",
    "* 1. 10 episodes are iterated through via for loop.  This is for our baseline random agent. \n",
    "* 2. We initialize each episode\n",
    "* 3. We perform action until we are done for each game episode\n",
    "    * 1. We render the environment\n",
    "    * 2. We take a random action and store the next state, reward, done (True or false) and info for each action taken\n",
    "    * 3. We accumulate our total score, which is a running sum of reward received for each action (through one complete episode)\n",
    "    * 4. We clear the output after each action to display the next screen. \n",
    "    * 5. After each epiosde, we display the total score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cfe6b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 9\n",
      "Score: -767\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "\n",
    "for episode in range (1, episodes):\n",
    "    state = env.reset()\n",
    "    done= False\n",
    "    score =0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        state, reward, done, info = env.step(env.action_space.sample())\n",
    "        score+=reward\n",
    "        clear_output(wait=True)\n",
    "    print ('Episode: {}\\nScore: {}'.format (episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb54679",
   "metadata": {},
   "source": [
    "# Q-Learning Agent: Q-learning is a reinforcement learning algorithm that searches the best possible next action for a state, which will maximize the sum of its near-term and long-term rewards. \n",
    "* In order to determine the value maximizing action for a  given state, agent will first need to explore the environment.  And keep track of all the values encountered in the environment. If only we could maintain a look up table of some sorts to keep track of state,action and associated value. Oh wait, academics already have figured this out and they have given it a fancy name **\"Q-table\"**.  The process of filling in a Q-table is called Q-Learning. \n",
    "\n",
    "* First we initiatlize the Q-table. Initialize means declare the size of the state and action pair. In this environment, there are 500 states and 6 actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9a30a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 6)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = env.action_space.n\n",
    "state = env.observation_space.n\n",
    "\n",
    "# initialized our Q-table\n",
    "q_table = np.zeros((state, actions))\n",
    "q_table.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310714f4",
   "metadata": {},
   "source": [
    "# Next we start the Q-learning algorithm, which will fill in the Q-table for us. \n",
    "# Q-learning algorithm five steps\n",
    "* 1. Initialize Q-table with zeroes\n",
    "* 2. Choose a random action in a state\n",
    "* 3. Perform the action\n",
    "* 4. Record value observed for that state, action pair. Value is calculated using the Bellman Equation. Bellman Equation is a famous optimization equation which is solved by maximizing = current reward + discounted (Future rewards)\n",
    "* 5. Go back to step 2\n",
    "\n",
    "* After doing these steps many times, you will update the q-table for each state,action pair with values observed. \n",
    "* steps for this algorithm are best explained in this note: \n",
    "https://www.freecodecamp.org/news/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b4e3b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for Q-learning\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "learning_rate =0.9\n",
    "discount_rate = 0.99\n",
    "exploration_rate=1\n",
    "max_exploration_rate =1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate=0.001\n",
    "rewards_all_episodes=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "45ff50cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Training Finished******\n"
     ]
    }
   ],
   "source": [
    "#Q-learning Algorithm\n",
    "rewards_all_episodes=[]\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        \n",
    "        #Exploration vs Exploitation trade-off\n",
    "        exploration_threshold=np.random.uniform(0,1)\n",
    "        if exploration_threshold>exploration_rate:\n",
    "            action=np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        #Update Q-Table\n",
    "        q_table[state, action] = q_table[state,action]*(1-learning_rate)+learning_rate*(reward + discount_rate*np.max(q_table[new_state,:]))\n",
    "        state=new_state\n",
    "        rewards_current_episode +=reward\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "                \n",
    "    exploration_rate =min_exploration_rate+ \\\n",
    "                    (max_exploration_rate-min_exploration_rate)*np.exp(-exploration_decay_rate*episode)\n",
    "    \n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "print(\"***** Training Finished******\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1a3dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "743c7699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 7.44059051,  8.525849  ,  7.44059051,  8.525849  ,  9.6220697 ,\n",
       "        -0.474151  ],\n",
       "       [11.84784175, 12.97761793, 11.84784175, 12.97761793, 14.11880599,\n",
       "         3.97761793],\n",
       "       ...,\n",
       "       [14.11880599, 15.2715212 , 14.11880599, 12.97761793,  5.11880599,\n",
       "         5.11880599],\n",
       "       [ 9.6220697 , 10.72936333,  9.6220697 , 10.72936333,  0.6220697 ,\n",
       "         0.6220697 ],\n",
       "       [17.612     , 16.43588   , 17.612     , 18.8       ,  8.612     ,\n",
       "         8.612     ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4767ea88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average per thousand episodes\n",
      "1000 :-132.33899999999994\n",
      "2000 :-8.829999999999997\n",
      "3000 :2.8689999999999887\n",
      "4000 :5.827999999999976\n",
      "5000 :6.802999999999978\n",
      "6000 :7.019999999999972\n",
      "7000 :7.300999999999966\n",
      "8000 :7.467999999999965\n",
      "9000 :7.404999999999954\n",
      "10000 :7.582999999999961\n"
     ]
    }
   ],
   "source": [
    "#Calculate and print average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "print(\"Average per thousand episodes\")\n",
    "\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \":\" + str(sum(r/1000)))\n",
    "    count +=1000\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "51351b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "score: 10\n"
     ]
    }
   ],
   "source": [
    "#visualize agent\n",
    "\n",
    "import time\n",
    "env.close()\n",
    "\n",
    "for episode in range(3):\n",
    "    state=env.reset()\n",
    "    done=False\n",
    "    print(\"Episode is: \"+str(episode))\n",
    "    time.sleep(1)\n",
    "    rewards=0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        action = np.argmax(q_table[state,:])\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        state= new_state\n",
    "        rewards += reward\n",
    "\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "        \n",
    "            env.render()\n",
    "            print(f\"score: {rewards}\")\n",
    "\n",
    "            time.sleep(1)\n",
    "            break\n",
    "    \n",
    "            \n",
    "        \n",
    "        state=new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca226f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9cfe23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
